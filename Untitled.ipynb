{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3410da2-3fa9-433b-9586-d9b30536ab49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mohamed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Mohamed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Mohamed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (27481, 4)\n",
      "\n",
      "Column names: ['textID', 'text', 'selected_text', 'sentiment']\n",
      "\n",
      "Sample data:\n",
      "       textID  \\\n",
      "0  cb774db0d1   \n",
      "1  549e992a42   \n",
      "2  088c60f138   \n",
      "3  9642c003ef   \n",
      "4  358bd9e861   \n",
      "\n",
      "                                                                          text  \\\n",
      "0                                          I`d have responded, if I were going   \n",
      "1                                Sooo SAD I will miss you here in San Diego!!!   \n",
      "2                                                    my boss is bullying me...   \n",
      "3                                               what interview! leave me alone   \n",
      "4   Sons of ****, why couldn`t they put them on the releases we already bought   \n",
      "\n",
      "                         selected_text sentiment  \n",
      "0  I`d have responded, if I were going   neutral  \n",
      "1                             Sooo SAD  negative  \n",
      "2                          bullying me  negative  \n",
      "3                       leave me alone  negative  \n",
      "4                        Sons of ****,  negative  \n",
      "\n",
      "Missing values in each column:\n",
      "textID           0\n",
      "text             1\n",
      "selected_text    1\n",
      "sentiment        0\n",
      "dtype: int64\n",
      "\n",
      "Sentiment distribution:\n",
      "sentiment\n",
      "neutral     11118\n",
      "positive     8582\n",
      "negative     7781\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Preprocessing text data...\n",
      "\n",
      "Sample of preprocessed text:\n",
      "                                                                          text  \\\n",
      "0                                          I`d have responded, if I were going   \n",
      "1                                Sooo SAD I will miss you here in San Diego!!!   \n",
      "2                                                    my boss is bullying me...   \n",
      "3                                               what interview! leave me alone   \n",
      "4   Sons of ****, why couldn`t they put them on the releases we already bought   \n",
      "\n",
      "                           processed_text  \n",
      "0                      id responded going  \n",
      "1                 sooo sad miss san diego  \n",
      "2                            bos bullying  \n",
      "3                   interview leave alone  \n",
      "4  son couldnt put release already bought  \n",
      "\n",
      "Extracting features using TF-IDF...\n",
      "Training set size: 21984\n",
      "Testing set size: 5497\n",
      "\n",
      "Training and evaluating models...\n",
      "\n",
      "Naive Bayes Results:\n",
      "Accuracy: 0.6365\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.74      0.50      0.59      1562\n",
      "     neutral       0.56      0.76      0.65      2230\n",
      "    positive       0.72      0.60      0.66      1705\n",
      "\n",
      "    accuracy                           0.64      5497\n",
      "   macro avg       0.67      0.62      0.63      5497\n",
      "weighted avg       0.66      0.64      0.63      5497\n",
      "\n",
      "\n",
      "Logistic Regression Results:\n",
      "Accuracy: 0.6824\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.59      0.65      1562\n",
      "     neutral       0.62      0.75      0.68      2230\n",
      "    positive       0.76      0.68      0.72      1705\n",
      "\n",
      "    accuracy                           0.68      5497\n",
      "   macro avg       0.70      0.67      0.68      5497\n",
      "weighted avg       0.69      0.68      0.68      5497\n",
      "\n",
      "\n",
      "Linear SVC Results:\n",
      "Accuracy: 0.6720\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.63      0.66      1562\n",
      "     neutral       0.63      0.68      0.65      2230\n",
      "    positive       0.72      0.70      0.71      1705\n",
      "\n",
      "    accuracy                           0.67      5497\n",
      "   macro avg       0.68      0.67      0.67      5497\n",
      "weighted avg       0.67      0.67      0.67      5497\n",
      "\n",
      "\n",
      "The best performing model is Logistic Regression with an accuracy of 0.6824\n",
      "\n",
      "Predicting sentiment for new tweets:\n",
      "Tweet: 'I love this product, it's amazing!'\n",
      "Predicted sentiment: positive\n",
      "\n",
      "Tweet: 'This is the worst experience ever, terrible service.'\n",
      "Predicted sentiment: negative\n",
      "\n",
      "Tweet: 'I'm not sure how I feel about this, need more time to decide.'\n",
      "Predicted sentiment: negative\n",
      "\n",
      "\n",
      "Analyzing feature importance (top 20 words for each sentiment)...\n",
      "\n",
      "Sentiment analysis project completed!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('Tweets.csv')\n",
    "pd.set_option('display.max_colwidth', None)  # Show full content in each cell\n",
    "\n",
    "# Exploratory Data Analysis\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nColumn names:\", df.columns.tolist())\n",
    "print(\"\\nSample data:\")\n",
    "print(df.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values in each column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Sentiment distribution\n",
    "print(\"\\nSentiment distribution:\")\n",
    "sentiment_counts = df['sentiment'].value_counts()\n",
    "print(sentiment_counts)\n",
    "\n",
    "# Plot sentiment distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='sentiment', data=df)\n",
    "plt.title('Sentiment Distribution in Tweets')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.savefig('sentiment_distribution.png')\n",
    "plt.close()\n",
    "\n",
    "# Text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "        \n",
    "        # Remove user mentions\n",
    "        text = re.sub(r'@\\w+', '', text)\n",
    "        \n",
    "        # Remove hashtags (keep the text without #)\n",
    "        text = re.sub(r'#(\\w+)', r'\\1', text)\n",
    "        \n",
    "        # Remove punctuation\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        \n",
    "        # Remove numbers\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        \n",
    "        # Tokenization\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        \n",
    "        # Remove stopwords\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "        \n",
    "        # Lemmatization\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "        \n",
    "        # Rejoin tokens\n",
    "        return ' '.join(tokens)\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# Apply preprocessing to 'text' column\n",
    "print(\"\\nPreprocessing text data...\")\n",
    "df['processed_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# Display a sample of preprocessed text\n",
    "print(\"\\nSample of preprocessed text:\")\n",
    "print(df[['text', 'processed_text']].head())\n",
    "\n",
    "# Feature extraction using TF-IDF\n",
    "print(\"\\nExtracting features using TF-IDF...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X = tfidf_vectorizer.fit_transform(df['processed_text'])\n",
    "y = df['sentiment']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Testing set size: {X_test.shape[0]}\")\n",
    "\n",
    "# Function to evaluate models\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Generate classification report\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{model_name} Results:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=sorted(y.unique()), \n",
    "                yticklabels=sorted(y.unique()))\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.savefig(f'confusion_matrix_{model_name.replace(\" \", \"_\").lower()}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return model, accuracy\n",
    "\n",
    "# Train and evaluate models\n",
    "print(\"\\nTraining and evaluating models...\")\n",
    "\n",
    "# Naive Bayes\n",
    "nb_model, nb_accuracy = evaluate_model(\n",
    "    MultinomialNB(), \n",
    "    X_train, X_test, y_train, y_test, \n",
    "    \"Naive Bayes\"\n",
    ")\n",
    "\n",
    "# Logistic Regression\n",
    "lr_model, lr_accuracy = evaluate_model(\n",
    "    LogisticRegression(max_iter=1000), \n",
    "    X_train, X_test, y_train, y_test, \n",
    "    \"Logistic Regression\"\n",
    ")\n",
    "\n",
    "# Linear SVC\n",
    "svc_model, svc_accuracy = evaluate_model(\n",
    "    LinearSVC(max_iter=10000), \n",
    "    X_train, X_test, y_train, y_test, \n",
    "    \"Linear SVC\"\n",
    ")\n",
    "\n",
    "# Compare model performances\n",
    "models = [\"Naive Bayes\", \"Logistic Regression\", \"Linear SVC\"]\n",
    "accuracies = [nb_accuracy, lr_accuracy, svc_accuracy]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=models, y=accuracies)\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "for i, acc in enumerate(accuracies):\n",
    "    plt.text(i, acc + 0.01, f'{acc:.4f}', ha='center')\n",
    "plt.savefig('model_comparison.png')\n",
    "plt.close()\n",
    "\n",
    "# Select the best model\n",
    "best_model_index = accuracies.index(max(accuracies))\n",
    "best_model_name = models[best_model_index]\n",
    "if best_model_index == 0:\n",
    "    best_model = nb_model\n",
    "elif best_model_index == 1:\n",
    "    best_model = lr_model\n",
    "else:\n",
    "    best_model = svc_model\n",
    "\n",
    "print(f\"\\nThe best performing model is {best_model_name} with an accuracy of {max(accuracies):.4f}\")\n",
    "\n",
    "# Function to predict sentiment of new tweets\n",
    "def predict_sentiment(tweets, model=best_model, vectorizer=tfidf_vectorizer):\n",
    "    # Preprocess the tweets\n",
    "    processed_tweets = [preprocess_text(tweet) for tweet in tweets]\n",
    "    \n",
    "    # Transform the tweets using the same vectorizer\n",
    "    X_new = vectorizer.transform(processed_tweets)\n",
    "    \n",
    "    # Predict sentiment\n",
    "    predictions = model.predict(X_new)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Example usage: Predict sentiment of new tweets\n",
    "new_tweets = [\n",
    "    \"I love this product, it's amazing!\",\n",
    "    \"This is the worst experience ever, terrible service.\",\n",
    "    \"I'm not sure how I feel about this, need more time to decide.\"\n",
    "]\n",
    "\n",
    "print(\"\\nPredicting sentiment for new tweets:\")\n",
    "for tweet, sentiment in zip(new_tweets, predict_sentiment(new_tweets)):\n",
    "    print(f\"Tweet: '{tweet}'\\nPredicted sentiment: {sentiment}\\n\")\n",
    "\n",
    "# Feature importance analysis (for Logistic Regression)\n",
    "if lr_accuracy > 0:\n",
    "    print(\"\\nAnalyzing feature importance (top 20 words for each sentiment)...\")\n",
    "    \n",
    "    # Get feature names\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # For each sentiment, find the most important words\n",
    "    for sentiment_class in sorted(y.unique()):\n",
    "        if isinstance(lr_model.coef_, np.ndarray) and lr_model.coef_.shape[0] > 1:\n",
    "            # For multiclass problems\n",
    "            sentiment_index = np.where(lr_model.classes_ == sentiment_class)[0][0]\n",
    "            coef = lr_model.coef_[sentiment_index]\n",
    "        else:\n",
    "            # For binary problems\n",
    "            coef = lr_model.coef_[0]\n",
    "            if sentiment_class != lr_model.classes_[1]:\n",
    "                coef = -coef\n",
    "                \n",
    "        # Get top 20 words for this sentiment\n",
    "        top_indices = np.argsort(coef)[-20:]\n",
    "        top_words = [feature_names[i] for i in top_indices]\n",
    "        top_coefs = [coef[i] for i in top_indices]\n",
    "        \n",
    "        # Plot\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.barplot(x=top_coefs, y=top_words)\n",
    "        plt.title(f'Top 20 Words for {sentiment_class.capitalize()} Sentiment')\n",
    "        plt.xlabel('Coefficient')\n",
    "        plt.ylabel('Word')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'top_words_{sentiment_class}.png')\n",
    "        plt.close()\n",
    "\n",
    "print(\"\\nSentiment analysis project completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027ab5b8-831f-4767-bf59-d49b23fb3fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     self-signed certificate in certificate chain\n",
      "[nltk_data]     (_ssl.c:1006)>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     self-signed certificate in certificate chain\n",
      "[nltk_data]     (_ssl.c:1006)>\n",
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     self-signed certificate in certificate chain\n",
      "[nltk_data]     (_ssl.c:1006)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset shape: (27481, 4)\n",
      "\n",
      "Sample data:\n",
      "       textID  \\\n",
      "0  cb774db0d1   \n",
      "1  549e992a42   \n",
      "2  088c60f138   \n",
      "3  9642c003ef   \n",
      "4  358bd9e861   \n",
      "\n",
      "                                                                          text  \\\n",
      "0                                          I`d have responded, if I were going   \n",
      "1                                Sooo SAD I will miss you here in San Diego!!!   \n",
      "2                                                    my boss is bullying me...   \n",
      "3                                               what interview! leave me alone   \n",
      "4   Sons of ****, why couldn`t they put them on the releases we already bought   \n",
      "\n",
      "                         selected_text sentiment  \n",
      "0  I`d have responded, if I were going   neutral  \n",
      "1                             Sooo SAD  negative  \n",
      "2                          bullying me  negative  \n",
      "3                       leave me alone  negative  \n",
      "4                        Sons of ****,  negative  \n",
      "\n",
      "Missing values:\n",
      "textID           0\n",
      "text             1\n",
      "selected_text    1\n",
      "sentiment        0\n",
      "dtype: int64\n",
      "\n",
      "Sentiment distribution:\n",
      "sentiment\n",
      "neutral     11118\n",
      "positive     8582\n",
      "negative     7781\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Preprocessing text data...\n",
      "\n",
      "Sample of preprocessed text:\n",
      "                                                                          text  \\\n",
      "0                                          I`d have responded, if I were going   \n",
      "1                                Sooo SAD I will miss you here in San Diego!!!   \n",
      "2                                                    my boss is bullying me...   \n",
      "3                                               what interview! leave me alone   \n",
      "4   Sons of ****, why couldn`t they put them on the releases we already bought   \n",
      "\n",
      "                           processed_text  \n",
      "0                      id responded going  \n",
      "1                 sooo sad miss san diego  \n",
      "2                            bos bullying  \n",
      "3                   interview leave alone  \n",
      "4  son couldnt put release already bought  \n",
      "Training set size: 21984\n",
      "Testing set size: 5497\n",
      "\n",
      "Tokenizing text...\n",
      "\n",
      "Training class distribution: [6225 8894 6865]\n",
      "\n",
      "Applying SMOTE to balance classes...\n",
      "Resampled class distribution: [8894 8894 8894]\n",
      "\n",
      "Building LSTM model...\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 50, 200)           2000000   \n",
      "                                                                 \n",
      " spatial_dropout1d (Spatial  (None, 50, 200)           0         \n",
      " Dropout1D)                                                      \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 50, 256)           336896    \n",
      " al)                                                             \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirecti  (None, 128)               164352    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2509699 (9.57 MB)\n",
      "Trainable params: 2509699 (9.57 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "\n",
      "Training LSTM model...\n",
      "Epoch 1/20\n",
      " 14/209 [=>............................] - ETA: 13:38 - loss: 1.0988 - accuracy: 0.3426  "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the dataset\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv('Tweets.csv')\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"\\nSample data:\")\n",
    "print(df.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Sentiment distribution\n",
    "print(\"\\nSentiment distribution:\")\n",
    "sentiment_counts = df['sentiment'].value_counts()\n",
    "print(sentiment_counts)\n",
    "\n",
    "# Plot sentiment distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='sentiment', data=df)\n",
    "plt.title('Sentiment Distribution in Tweets')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.savefig('sentiment_distribution.png')\n",
    "plt.close()\n",
    "\n",
    "# Text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "        \n",
    "        # Remove user mentions\n",
    "        text = re.sub(r'@\\w+', '', text)\n",
    "        \n",
    "        # Remove hashtags (keep the text without #)\n",
    "        text = re.sub(r'#(\\w+)', r'\\1', text)\n",
    "        \n",
    "        # Remove punctuation\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        \n",
    "        # Remove numbers\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        \n",
    "        # Tokenization\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        \n",
    "        # Remove stopwords\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "        \n",
    "        # Lemmatization\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "        \n",
    "        # Rejoin tokens\n",
    "        return ' '.join(tokens)\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# Apply preprocessing to 'text' column\n",
    "print(\"\\nPreprocessing text data...\")\n",
    "df['processed_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# Display a sample of preprocessed text\n",
    "print(\"\\nSample of preprocessed text:\")\n",
    "print(df[['text', 'processed_text']].head())\n",
    "\n",
    "# Convert sentiment labels to numerical values\n",
    "label_mapping = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "df['sentiment_numeric'] = df['sentiment'].map(label_mapping)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X = df['processed_text'].values\n",
    "y = df['sentiment_numeric'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Testing set size: {len(X_test)}\")\n",
    "\n",
    "# Tokenize text\n",
    "print(\"\\nTokenizing text...\")\n",
    "max_words = 10000  # Maximum number of words to keep\n",
    "max_len = 50       # Maximum sequence length\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convert text to sequences\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post')\n",
    "\n",
    "# Check class distribution\n",
    "train_class_counts = np.bincount(y_train)\n",
    "print(f\"\\nTraining class distribution: {train_class_counts}\")\n",
    "\n",
    "# Apply SMOTE for class balancing\n",
    "print(\"\\nApplying SMOTE to balance classes...\")\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_pad_reshaped = X_train_pad.reshape(X_train_pad.shape[0], -1)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_pad_reshaped, y_train)\n",
    "X_train_resampled = X_train_resampled.reshape(X_train_resampled.shape[0], max_len)\n",
    "\n",
    "resampled_class_counts = np.bincount(y_train_resampled)\n",
    "print(f\"Resampled class distribution: {resampled_class_counts}\")\n",
    "\n",
    "# Build LSTM model\n",
    "print(\"\\nBuilding LSTM model...\")\n",
    "embedding_dim = 200\n",
    "\n",
    "# Create a more sophisticated model with Bidirectional LSTM\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=max_len))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))  # 3 classes: negative, neutral, positive\n",
    "\n",
    "# Compile model with a lower learning rate\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Display model summary\n",
    "model.summary()\n",
    "\n",
    "# Define callbacks for early stopping and model checkpoint\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True),\n",
    "    ModelCheckpoint('best_lstm_model.h5', monitor='val_accuracy', save_best_only=True, mode='max')\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "print(\"\\nTraining LSTM model...\")\n",
    "history = model.fit(\n",
    "    X_train_resampled, y_train_resampled,\n",
    "    validation_data=(X_test_pad, y_test),\n",
    "    epochs=20,\n",
    "    batch_size=128,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\nEvaluating model...\")\n",
    "loss, accuracy = model.evaluate(X_test_pad, y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_prob = model.predict(X_test_pad)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Generate classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "target_names = ['negative', 'neutral', 'positive']\n",
    "report = classification_report(y_test, y_pred, target_names=target_names)\n",
    "print(report)\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)\n",
    "plt.title('Confusion Matrix - LSTM Model')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.savefig('confusion_matrix_lstm.png')\n",
    "plt.close()\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history_lstm.png')\n",
    "plt.close()\n",
    "\n",
    "# Function to predict sentiment of new tweets\n",
    "def predict_sentiment(tweets, model=model, tokenizer=tokenizer, max_len=max_len):\n",
    "    # Preprocess the tweets\n",
    "    processed_tweets = [preprocess_text(tweet) for tweet in tweets]\n",
    "    \n",
    "    # Convert to sequences\n",
    "    sequences = tokenizer.texts_to_sequences(processed_tweets)\n",
    "    \n",
    "    # Pad sequences\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
    "    \n",
    "    # Predict sentiment\n",
    "    predictions_prob = model.predict(padded_sequences)\n",
    "    predictions = np.argmax(predictions_prob, axis=1)\n",
    "    \n",
    "    # Map numerical predictions back to sentiment labels\n",
    "    reverse_mapping = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "    sentiment_predictions = [reverse_mapping[pred] for pred in predictions]\n",
    "    confidence_scores = [np.max(prob) for prob in predictions_prob]\n",
    "    \n",
    "    return sentiment_predictions, confidence_scores\n",
    "\n",
    "# Example usage: Predict sentiment of new tweets\n",
    "new_tweets = [\n",
    "    \"I love this product, it's amazing!\",\n",
    "    \"This is the worst experience ever, terrible service.\",\n",
    "    \"I'm not sure how I feel about this, need more time to decide.\"\n",
    "]\n",
    "\n",
    "print(\"\\nPredicting sentiment for new tweets:\")\n",
    "sentiments, confidences = predict_sentiment(new_tweets)\n",
    "for tweet, sentiment, confidence in zip(new_tweets, sentiments, confidences):\n",
    "    print(f\"Tweet: '{tweet}'\")\n",
    "    print(f\"Predicted sentiment: {sentiment} (Confidence: {confidence:.4f})\\n\")\n",
    "\n",
    "# Save tokenizer for future use\n",
    "import pickle\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(\"\\nSentiment analysis with LSTM completed!\")\n",
    "\n",
    "# Optional: Implement an ensemble model for higher accuracy\n",
    "print(\"\\nCreating an ensemble model for even higher accuracy...\")\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Prepare traditional ML features\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Train traditional ML models\n",
    "print(\"Training traditional ML models for ensemble...\")\n",
    "nb_model = MultinomialNB().fit(X_train_tfidf, y_train)\n",
    "lr_model = LogisticRegression(max_iter=1000).fit(X_train_tfidf, y_train)\n",
    "svc_model = LinearSVC(max_iter=10000).fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Get LSTM predictions for training set\n",
    "lstm_train_preds = model.predict(X_train_pad)\n",
    "lstm_train_preds_class = np.argmax(lstm_train_preds, axis=1)\n",
    "\n",
    "# Create a meta-classifier\n",
    "meta_features_train = np.column_stack([\n",
    "    nb_model.predict_proba(X_train_tfidf)[:, 1],  # NB probability for class 1\n",
    "    lr_model.decision_function(X_train_tfidf),    # LR decision function\n",
    "    lstm_train_preds                             # LSTM probabilities\n",
    "])\n",
    "\n",
    "meta_classifier = LogisticRegression(max_iter=1000)\n",
    "meta_classifier.fit(meta_features_train, y_train)\n",
    "\n",
    "# Make predictions with all models on test set\n",
    "nb_preds = nb_model.predict_proba(X_test_tfidf)[:, 1]\n",
    "lr_preds = lr_model.decision_function(X_test_tfidf)\n",
    "lstm_preds = model.predict(X_test_pad)\n",
    "\n",
    "# Combine predictions for meta-classifier\n",
    "meta_features_test = np.column_stack([nb_preds, lr_preds, lstm_preds])\n",
    "ensemble_preds = meta_classifier.predict(meta_features_test)\n",
    "\n",
    "# Evaluate ensemble model\n",
    "ensemble_accuracy = accuracy_score(y_test, ensemble_preds)\n",
    "print(f\"\\nEnsemble Model Accuracy: {ensemble_accuracy:.4f}\")\n",
    "\n",
    "# Generate classification report for ensemble\n",
    "print(\"\\nEnsemble Classification Report:\")\n",
    "ensemble_report = classification_report(y_test, ensemble_preds, target_names=target_names)\n",
    "print(ensemble_report)\n",
    "\n",
    "# Generate confusion matrix for ensemble\n",
    "ensemble_cm = confusion_matrix(y_test, ensemble_preds)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(ensemble_cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)\n",
    "plt.title('Confusion Matrix - Ensemble Model')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.savefig('confusion_matrix_ensemble.png')\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nSentiment analysis project with LSTM and ensemble modeling completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b313127-9043-418d-af27-34263aa8e6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mohamed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Mohamed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Mohamed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset shape: (27481, 4)\n",
      "\n",
      "Sample data:\n",
      "       textID  \\\n",
      "0  cb774db0d1   \n",
      "1  549e992a42   \n",
      "2  088c60f138   \n",
      "3  9642c003ef   \n",
      "4  358bd9e861   \n",
      "\n",
      "                                                                          text  \\\n",
      "0                                          I`d have responded, if I were going   \n",
      "1                                Sooo SAD I will miss you here in San Diego!!!   \n",
      "2                                                    my boss is bullying me...   \n",
      "3                                               what interview! leave me alone   \n",
      "4   Sons of ****, why couldn`t they put them on the releases we already bought   \n",
      "\n",
      "                         selected_text sentiment  \n",
      "0  I`d have responded, if I were going   neutral  \n",
      "1                             Sooo SAD  negative  \n",
      "2                          bullying me  negative  \n",
      "3                       leave me alone  negative  \n",
      "4                        Sons of ****,  negative  \n",
      "\n",
      "Missing values:\n",
      "textID           0\n",
      "text             1\n",
      "selected_text    1\n",
      "sentiment        0\n",
      "dtype: int64\n",
      "\n",
      "Sentiment distribution:\n",
      "sentiment\n",
      "neutral     11118\n",
      "positive     8582\n",
      "negative     7781\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Preprocessing text data...\n",
      "\n",
      "Sample of preprocessed text:\n",
      "                                                                          text  \\\n",
      "0                                          I`d have responded, if I were going   \n",
      "1                                Sooo SAD I will miss you here in San Diego!!!   \n",
      "2                                                    my boss is bullying me...   \n",
      "3                                               what interview! leave me alone   \n",
      "4   Sons of ****, why couldn`t they put them on the releases we already bought   \n",
      "\n",
      "                           processed_text  \n",
      "0                      id responded going  \n",
      "1                 sooo sad miss san diego  \n",
      "2                            bos bullying  \n",
      "3                   interview leave alone  \n",
      "4  son couldnt put release already bought  \n",
      "Training set size: 21984\n",
      "Testing set size: 5497\n",
      "\n",
      "Tokenizing text...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pickle\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the dataset\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv('Tweets.csv')\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"\\nSample data:\")\n",
    "print(df.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Sentiment distribution\n",
    "print(\"\\nSentiment distribution:\")\n",
    "sentiment_counts = df['sentiment'].value_counts()\n",
    "print(sentiment_counts)\n",
    "\n",
    "# Plot sentiment distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='sentiment', data=df)\n",
    "plt.title('Sentiment Distribution in Tweets')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.savefig('sentiment_distribution.png')\n",
    "plt.close()\n",
    "\n",
    "# Text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "        \n",
    "        # Remove user mentions\n",
    "        text = re.sub(r'@\\w+', '', text)\n",
    "        \n",
    "        # Remove hashtags (keep the text without #)\n",
    "        text = re.sub(r'#(\\w+)', r'\\1', text)\n",
    "        \n",
    "        # Remove punctuation\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        \n",
    "        # Remove numbers\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        \n",
    "        # Tokenization\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        \n",
    "        # Remove stopwords\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "        \n",
    "        # Lemmatization\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "        \n",
    "        # Rejoin tokens\n",
    "        return ' '.join(tokens)\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# Apply preprocessing to 'text' column\n",
    "print(\"\\nPreprocessing text data...\")\n",
    "df['processed_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# Display a sample of preprocessed text\n",
    "print(\"\\nSample of preprocessed text:\")\n",
    "print(df[['text', 'processed_text']].head())\n",
    "\n",
    "# Convert sentiment labels to numerical values\n",
    "label_mapping = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "df['sentiment_numeric'] = df['sentiment'].map(label_mapping)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X = df['processed_text'].values\n",
    "y = df['sentiment_numeric'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Testing set size: {len(X_test)}\")\n",
    "\n",
    "# Tokenize text\n",
    "print(\"\\nTokenizing text...\")\n",
    "max_words = 10000  # Maximum number of words to keep\n",
    "max_len = 50       # Maximum sequence length\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convert text to sequences\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "with open('tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4dc79e-23c8-46cc-ac30-795b239ac943",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
